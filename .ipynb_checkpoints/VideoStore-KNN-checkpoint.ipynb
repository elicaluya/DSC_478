{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this example we will look at how to use the K-Nearest_Neighbor algorithm for classification. We will use a modified version of the Video Store data set for this example. We will use the \"Incidentals\" attribute as the target attribute for classification (the class attribute). The goal is to be able to classify an unseen instance as \"Yes\" or \"No\" given the values of \"Incidentals\" from training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vstable = pd.read_csv(\"../data/Video_Store_2.csv\", index_col=0)\n",
    "\n",
    "vstable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vstable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will be splitting the data into a test and training partions with the test partition to be used for evaluating model error-rate and the training partition to be used to find the K nearest neighbors. Before spliting the data we need to do a random reshuffling to make sure the instances are randomized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = vstable.reindex(np.random.permutation(vstable.index))\n",
    "vs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_names = vs.columns.values\n",
    "vs_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The target attribute for classification is Incidentals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_target = vs.Incidentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before we can compute distances we need to convert the data (excluding the target attribute containing the class labels) into standard spreadsheet format with binary dummy variables for categorical attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = pd.get_dummies(vs[['Gender','Income','Age','Rentals','Avg Per Visit','Genre']])\n",
    "vs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To be able to evaluate the accuracy of our predictions, we will split the data into training and test sets. In this case, we will use 80% for training and the remaining 20% for testing. Note that we must also do the same split to the target attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpercent = 0.8\n",
    "tsize = int(np.floor(tpercent * len(vs)))\n",
    "vs_train = vs[:tsize]\n",
    "vs_test = vs[tsize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (vs_train.shape)\n",
    "print (vs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, linewidth=120)\n",
    "\n",
    "vs_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the target attribute (\"Incidentals\") accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_target_train = vs_target[0:int(tsize)]\n",
    "vs_target_test = vs_target[int(tsize):len(vs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_target_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_target_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we normalize the attributes so that everything is in [0,1] scale. We can use the normalization functions from the kNN module in Ch. 2 of the text. In this case, however, we will use the more flexible and robust scaler function from the preprocessing module of scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "min_max_scaler.fit(vs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_train_norm = min_max_scaler.fit_transform(vs_train)\n",
    "vs_test_norm = min_max_scaler.fit_transform(vs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that while these Scikit-learn functions accept Pandas dataframes as input, they return Numpy arrays (in this case the normalized versions of vs_train and vs_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, linewidth=100)\n",
    "vs_train_norm[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For consitency, we'll also convert the training and test target labels into Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_target_train = np.array(vs_target_train)\n",
    "vs_target_test = np.array(vs_target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (vs_target_train)\n",
    "print (\"\\n\")\n",
    "print (vs_target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following function illustrates how we can perform a k-nearest-neighbor search. The \"measure\" argument allows us to use either Euclidean distance or (the inverse of) Cosine similarity as the distance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_search(x, D, K, measure):\n",
    "    \"\"\" find K nearest neighbors of an instance x among the instances in D \"\"\"\n",
    "    if measure == 0:\n",
    "        # euclidean distances from the other points\n",
    "        dists = np.sqrt(((D - x)**2).sum(axis=1))\n",
    "    elif measure == 1:\n",
    "        # first find the vector norm for each instance in D as wel as the norm for vector x\n",
    "        D_norm = np.array([np.linalg.norm(D[i]) for i in range(len(D))])\n",
    "        x_norm = np.linalg.norm(x)\n",
    "        # Compute Cosine: divide the dot product o x and each instance in D by the product of the two norms\n",
    "        sims = np.dot(D,x)/(D_norm * x_norm)\n",
    "        # The distance measure will be the inverse of Cosine similarity\n",
    "        dists = 1 - sims\n",
    "    idx = np.argsort(dists) # sorting\n",
    "    # return the indexes of K nearest neighbors\n",
    "    return idx[:K], dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use vs_test_norm[0] as a test instance x and find its K nearest neighbors\n",
    "neigh_idx, distances = knn_search(vs_test_norm[0], vs_train_norm, 5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (neigh_idx)\n",
    "print (\"\\n\")\n",
    "vs_train.iloc[neigh_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (distances[neigh_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_labels = vs_target_train[neigh_idx]\n",
    "print (neigh_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we know the nearest neighbors, we need to find the majority class label among them. The majority class would be the class assgined to the new instance x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print (Counter(neigh_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(neigh_labels).most_common(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we'll use the Knn module from Chapter 2 of Machine Learning in Action. Before importing the whole module, let's illustrate what the code does by stepping through it with some specific input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetSize = vs_train_norm.shape[0]\n",
    "print (dataSetSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inX = vs_test_norm[0]   # We'll use the first instance in the test data for this example\n",
    "diffMat = np.tile(inX, (dataSetSize,1)) - vs_train_norm  # Create dataSetSize copies of inX, as rows of a 2D matrix\n",
    "                                                         # Compute a matrix of differences\n",
    "print (diffMat[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqDiffMat = diffMat**2  # The matrix of squared differences\n",
    "sqDistances = sqDiffMat.sum(axis=1)  # 1D array of the sum of squared differences (one element for each training instance)\n",
    "distances = sqDistances**0.5  # and finally the matrix of Euclidean distances to inX\n",
    "print (distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedDistIndicies = distances.argsort() # the indices of the training instances in increasing order of distance to inX\n",
    "print (sortedDistIndicies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To see how the test instance should be classified, we need to find the majority class among the neighbors (here we do not use distance weighting; only a simply voting approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classCount={}\n",
    "k = 5  # We'll use the top 10 neighbors\n",
    "for i in range(k):\n",
    "   voteIlabel = vs_target_train[sortedDistIndicies[i]]\n",
    "   classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1  # add to the count of the label or retun 1 for first occurrence\n",
    "   print (sortedDistIndicies[i], voteIlabel, classCount[voteIlabel])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's find the predicted class for the test instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "# Create a dictionary for the class labels with cumulative occurrences across the neighbors as values\n",
    "# Dictionary will be ordered in decreasing order of the lable values (so the majority class label will\n",
    "# be the first dictonary element)\n",
    "sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print (sortedClassCount)\n",
    "print (sortedClassCount[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A better way to find the majority class given a list of class labels from neighbors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "k = 5  # We'll use the top 5 neighbors\n",
    "vote = vs_target_train[sortedDistIndicies[0:k]]\n",
    "maj_class = Counter(vote).most_common(1)\n",
    "\n",
    "print (vote)\n",
    "\n",
    "print (maj_class)\n",
    "\n",
    "print (\"Class label for the classified istance: \", maj_class[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's know import the whole KNN module from MLA (uploaded on D2L) and use as part of a more robust evaluation process. We will step through all test instances, use our Knn classifier to predict a class label for each instance, and in each case we compare the predicted label to the actual value from the target test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTestVecs = len(vs_target_test)\n",
    "print (numTestVecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorCount = 0.0\n",
    "for i in range(numTestVecs):\n",
    "    # classify0 function uses Euclidean distance to find k nearest neighbors\n",
    "    classifierResult = kNN.classify0(vs_test_norm[i,:], vs_train_norm, vs_target_train, 3)\n",
    "    print (\"the classifier came back with: %s, the real answer is: %s\" % (classifierResult, vs_target_test[i]))\n",
    "    if (classifierResult != vs_target_test[i]): errorCount += 1.0\n",
    "        \n",
    "print (\"the total error rate is: %f\" % (errorCount/float(numTestVecs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I have added a new classifier function to the kNN module that uses Cosine similarity instead of Euclidean distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(kNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorCount = 0.0\n",
    "for i in range(numTestVecs):\n",
    "    # classify1 function uses inverse of Cosine similarity to find k nearest neighbors\n",
    "    classifierResult2 = kNN.classify1(vs_test_norm[i,:], vs_train_norm, vs_target_train, 3)\n",
    "    print (\"the classifier came back with: %s, the real answer is: %s\" % (classifierResult2, vs_target_test[i]))\n",
    "    if (classifierResult2 != vs_target_test[i]): errorCount += 1.0\n",
    "print (\"the total error rate is: %f\" % (errorCount/float(numTestVecs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
